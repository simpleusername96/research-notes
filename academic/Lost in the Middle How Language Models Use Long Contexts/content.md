2024.07.04

[LinkedIn](https://www.linkedin.com/posts/byeongheon-lee-2b83aa222_lost-in-the-middle-activity-7219735971578138624-VcMq?utm_source=share&utm_medium=member_desktop&rcm=ACoAADfxcywBkH2Mi2-YPZm7jSZERa3dQ2_DDEY)
[Medium](https://medium.com/@simple0314/lost-in-the-middle-0313264b35d0)

LLM 기반 챗봇을 사용하면서 긴 참고자료에 대한 요약을 요청할 때, 정보가 생략된 것 같은 경험을 해본 적이 많을 것입니다. 이는 환각(hallucination) 현상과 더불어 사용자 경험에 큰 영향을 미치는 문제 중 하나입니다. 이렇게 LLM이 프롬프트 안의 정보를 충분히 활용하지 못하는 현상을 'Lost-in-the-Middle'이라고 부릅니다.

@GregKamradt의 X 포스트를(https://x.com/GregKamradt/status/1722386725635580292) 통해 이 현상에 대해 처음 알게 되었습니다. 이 게시물에서는 GPT-4-128K가 실제로 주장하는 것처럼 128K의 컨텍스트 윈도우 안에 있는 모든 정보를 활용하는지 테스트하기 위해 'Needle-in-a-Haystack' 방법을 소개했습니다. 이 테스트는 Paul Graham의 에세이들을 취합하여 최대 128K 토큰에 가까운 긴 문서를 준비하고, 문서 내에 "The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day."라는 전체 에세이 맥락과 무관한 문장을 삽입하는 방식으로 진행되었습니다. 테스트 결과, 73K 토큰 길이의 문서를 제공했을 때는 성공적으로 문장을 인지했지만, 그 이후에는 프롬프트의 7%에서 50% 깊이에 위치한 경우 문장을 찾지 못했습니다. 
Claude 2.1이 출시된 후에도 사람들이 동일한 테스트를 적용했을 때 매우 낮은 성능을 보여 논란이 있기도 했습니다. 그러나 Anthropic에서 빠른 시일 내에 관련 블로그 글(https://anthropic.com/news/claude-2-1-prompting)을 게시하여 "Here is the most relevant sentence in the context:"라는 프롬프트를 사용해 해당 이슈를 완화할 수 보여주며 이를 잠재우기도 했죠. 이처럼 LLM의 성능 평가에서 long context를 얼마나 잘 활용할 수 있는지는 중요한 요소로 자리 잡게 되었습니다. 앞으로 몇 차례에 나눠서 관련된 연구에 대해 정리해보려고 합니다.


1. Lost in the Middle: How Language Models Use Long Contexts
- 가장 처음으로 해당 이슈를 넓리 알린 연구인 것 같습니다. 인용수도 2024.07.04 기준 400회를 훌쩍 넘기기도 했구요.
- LLM에게 제공하는 프롬프트의 길이를 기준으로 0%를 가장 첫 부분, 100%를 가장 나중 부분이라고 하면, 중간 부분의 내용을 활용하지 못하는 것을 그래프로 나타내 u-shape performance curve라는 현상이 관련 연구에서는 지속적으로 쓰이는 표현이 되는 것 같습니다.


1. 서론
최근 언어 모델의 context window가 증가하고 있지만, 이러한 모델들이 실제로 긴 프롬프트를 얼마나 잘 활용하는지에 대한 연구는 부족한 상황입니다. 연구진은 다중 문서 질의응답(Multi-document question answering)과 키-값 검색(Key-value retrieval)을 통해 이를 분석했습니다.

주요 발견은 다음과 같습니다:
- 정답 생성에 필요한 정보의 프롬프트 내 위치를 변경하면 모델의 성능이 크게 저하될 수 있습니다.
- 모델은 입력 프롬프트의 시작 또는 끝 부분에 있는 정보를 가장 잘 활용하며 프롬프트의 중간에 위치한 정보를 사용해야 할 때 성능이 크게 떨어집니다.
- 기본 모델에서 context window를 확장한 모델들이라고 해도 증가한 context window 내의 정보를 충분히 활용하고 있지 못하고 있습니다.

1. 다중 문서 질의응답 실험
2.1 실험 구성
- 데이터셋: NaturalQuestions-Open
- 입력
	- 질문
	- k개의 문서 (Wikipedia 문단)
		- 단 하나의 문서만 답변을 포함
		- 나머지 k-1개는 질문과 관련이 있지만 답변을 포함하지는 않는  distractor 문서
- 목적: 모델은 답변이 포함된 문서를 찾아 질문에 답해야 함
- 실험 조건 조절 
	- 답변이 포함된 문서의 입력 프롬프트 내 위치 변경
	- 답변이 없는 distractor 문서 수를 조절해 입력 프롬프트의 길이를 변경

2.2 평가 모델
오픈소스 모델:
MPT-30B-Instruct (최대 문맥 길이: 8192 토큰)
LongChat-13B(16K) (LLaMA-13B의 context window 확장 모델, 최대 문맥 길이: 16384 토큰)
상용 모델:
GPT-3.5-Turbo (최대 문맥 길이: 4K 토큰)
GPT-3.5-Turbo (16K) (최대 문맥 길이: 16K 토큰)
Claude-1.3 (최대 문맥 길이: 8K 토큰)
Claude-1.3 (100K) (최대 문맥 길이: 100K 토큰)

2.3 결과 및 논의
주요 실험 결과는 다음과 같습니다:
- U자형 성능 곡선
	- 모델들은 관련 정보가 입력 프롬프트의 시작 또는 끝에 있을 때 가장 높은 성능을 보임
	- 중간에 위치한 정보를 사용해야 할 때 성능이 크게 저하됨.
- 성능 저하 정도
	- 일부 경우, 성능 저하가 20% 이상에 달함.
	- 극단적인 경우, 문서 없이 질문에만 답변하는 것(closed-book)보다 더 낮은 성능을 보임
- 확장된 모델의 한계
	- 더 긴 context window를 가진 확장 모델(예: GPT-3.5-Turbo 16K)도 기본 모델과 거의 동일한 성능을 보임
	- 이는 확장된 모델이 긴 입력을 잘 활용하는 것은 아님을 시사함
	
- Retrieved distractor 문서들은 질문과 어느 정도 의미론적으로 관련이 있지만 답변을 하는 데 필수적인 정보는 결여된 상태입니다. 따라서 이와 관련된 질의응답 실험은 언어 모델에게 더 어렵다고 할 수 있습니다. 연구진은 Appendix B에서 모델의 순수한 정보 활용 능력을 평가하고자 했습니다.
	- 방법: 질문과 전혀 상관 없는 Wikipedia 문단들을 무작위로 선택 후 문서로 제공
	- 결과:
	1. 모든 모델의 전반적인 정확도가 향상됨
	2. 그러나 여전히 U자형 성능 곡선이 관찰됨
	- 의의: 모델의 성능 저하가 단순히 관련 문서 식별의 어려움 때문이 아니라, 긴 입력 프롬프트를 처리하는 능력의 한계 때문임을 시사
	
- 연구진은 Retrieved 문서들이 관련도 순으로 정렬되어 있다는 사전 지식이 모델의 성능에 영향을 미칠 수 있다고 판단했습니다. 이에 Appendix C에서는 distractor 문서의 순서를 무작위화하는 실험을 진행했습니다.
    - 방법: 
        1. 프롬프트에 "검색 결과가 무작위로 정렬되어 있습니다"라는 문구 추가
        2. k-1개의 distractor 문서 순서를 무작위로 섞음
    - 결과:
        1. U자형 성능 곡선이 여전히 관찰됨
        2. 입력 맥락의 중간과 끝부분에 있는 정보를 사용할 때 성능이 약간 향상됨
        3. 맥락의 시작 부분에 관련 정보가 있을 때 성능이 약간 감소함
    - 의의: 검색 결과의 순서에 대한 사전 지식이 모델의 성능에 일부 영향을 미치지만, 전반적인 패턴은 유지됨을 확인
	
- 연구진은 최신 대규모 언어 모델인 GPT-4의 성능을 평가하기 위해 Appendix D에서 추가 실험을 진행했습니다.
    - 방법: 
        1. GPT-4 (8K) 모델 사용
        2. 다중 문서 질의응답 실험의 일부(500개의 무작위 예제)에 대해 평가
        3. 20개의 문서가 포함된 입력 프롬프트 사용
    - 결과:
        1. GPT-4는 다른 모델들보다 높은 절대적 성능을 보임
        2. 그러나 여전히 U자형 성능 곡선이 관찰됨
        3. 입력 맥락의 시작이나 끝에 관련 정보가 있을 때 성능이 가장 높음
        4. 중간에 있는 정보를 사용해야 할 때 성능이 저하됨
    - 의의: 더 큰 규모의 최신 모델도 긴 입력 맥락을 효과적으로 활용하는 데 여전히 한계가 있음을 시사
	
1. 키-값 검색 실험
다중 문서 질의응답 실험에서 모델들이 중간에 위치한 정보를 잘 활용하지 못하는 것을 발견한 연구진은, 모델들의 '기본적인 검색 능력'을 더 단순화된 환경에서 테스트하기 위해 이 실험을 설계했습니다.

3.1 실험 구성
- 입력
	- k개의 키-값 쌍을 포함하는 텍스트 형태의 JSON 데이터
	- 검색할 키
- 목표
	- 주어진 키에 해당하는 값을 반환
- 특징
	- 모든 키와 값은 고유한 랜덤 UUID
	- 자연어 대신 무작위 문자열을 사용
- 실험 조건 조절
	- JSON 데이터 내 검색할 키의 위치를 변경
	- 키-값 쌍의 수(k)를 조절해 입력 프롬프트의 길이를 변경

3.2 결과 및 논의
- 모델 간 성능 차이
	- Claude-1.3과 Claude-1.3 (100K)는 모든 평가된 입력 프롬프트 길이에서 거의 완벽한 성능을 보임
	- 다른 모델들은 특히 140개 또는 300개의 키-값 쌍이 있는 경우 어려움을 겪음
- U자형 성능 곡선 재현:
	- GPT-3.5-Turbo, GPT-3.5-Turbo (16K), MPT-30B-Instruct는 다중 문서 질의응답 결과와 유사하게 입력 프롬프트의 중간에 있는 키-값 쌍에 접근할 때 가장 낮은 성능을 보임
- LongChat-13B (16K)의 특이 행동:
	- 140 키-값 쌍 설정에서 다른 패턴을 보임
	- 관련 정보가 입력 프롬프트의 시작에 있을 때, 값을 직접 출력하는 대신 키를 검색하는 코드를 생성하는 경향이 있음

1. 원인 분석
연구진은 언어 모델이 긴 입력 문맥을 효과적으로 활용하지 못하는 원인을 분석하기 위해 세 가지 요소를 조사했습니다.
- 모델 아키텍처(디코더 vs 인코더-디코더)
- Query-aware contextualization
- Instruction fine-tuning

4.1 모델 아키텍처의 영향
디코더 전용 모델과 인코더-디코더 모델을 비교했습니다.
- 실험 모델
	- 인코더-디코더 모델(Flan-T5-XXL, Flan-UL2)
- 주요 발견
  - 최대 context window 내에서 평가할 때, 인코더-디코더 모델은 관련 정보의 위치 변화에 상대적으로 강건했습니다. (예: Flan-UL2의 경우 최악과 최선의 성능 차이가 1.9%에 불과)
  - 그러나 context window보다 긴 시퀀스에서 평가할 때, 인코더-디코더 모델도 U자형 성능 곡선을 보였습니다.
- 연구진은 인코더-디코더 모델이 양방향 인코더를 사용해 context window 내에 위치한 정보에 대해서는 위치와 상관없이 활용할 수 있었을 것이라 추측했습니다.

4.2 Query-aware contextualization
쿼리(질문 또는 검색할 키)를 문서 전후에 배치하여 Query-aware contextualization의 효과를 테스트했습니다.
- 키-값 검색 작업에서 극적인 성능 향상
	- 모든 모델이 75, 140, 300 키-값 쌍에서 거의 완벽한 성능을 달성
- 다중 문서 질의응답 작업에서는 제한적인 영향
	- 입력 프롬프트 시작 부분에 관련 문서가 위치한 경우 성능이 약간 향상
	- 다른 위치에서는 성능이 약간 감소
- 해당 접근 방법은 단순한 검색 작업에는 효과적일 수 있지만, 더 복잡한 추론이 필요한 작업에서는 제한적인 영향을 미칠 수 있음을 시사합니다.

4.3 Instruction fine-tuning
Instruction Fine-tuning이 모델의 긴 입력 프롬프트 활용 능력에 미치는 영향을 조사하기 위해 MPT-30B-Instruct와 그 기본 모델 MPT-30B를 비교했습니다. 
- 주요 발견
	- 기본 모델(MPT-30B)과 Instruction fine-tuned 모델(MPT-30B-Instruct) 모두 U자형 성능 곡선을 보임
	- Instruction fine-tuned 모델의 절대적 성능이 전반적으로 더 높았음
	- Instruction fine-tuning은 최악의 경우 성능 격차를 약간 줄임 (기본 모델의 10%에서 약 4%로)
- 이 결과는 Instruction fine-tuning 과정 자체가 이러한 성능 패턴의 주요 원인은 아님을 시사합니다. 연구진은 non Instruction fine-tuned 언어 모델도 인터넷 텍스트(예: StackOverflow의 질문과 답변)에서 유사한 형식의 데이터를 통해 긴 입력 프롬프트를 사용하는 법을 학습했을 수 있다고 추측합니다.
- 추가적으로, Llama-2 모델들(7B, 13B, 70B)에 대한 실험에서 다음과 같은 점을 발견했습니다.
	- U자형 성능 곡선은 충분히 큰 모델에서만 나타남 (13B 이상)
	- 작은 모델(7B)은 오직 입력 프롬프트의 끝에 관련 문서가 있을 때 정답률이 높은 recency bias을 보임
	- 추가적인 fine-tuning과 RLHF는 작은 모델(13B)에서는 위치 편향을 약간 완화했지만, 큰 모델(70B)에서는 거의 영향을 미치지 않음

1. 더 긴 프롬프트가 항상 좋은가?
- 연구진은 언어 모델에 더 많은 프롬프트를 제공하는 것이 downstream task의 성능 향상으로 이어지는지, 그리고 추가 정보와 증가된 추론 복잡성 사이의 trade-off를 조사하고자 이 실험을 수행했습니다. 
- 이를 검증하기 위해 연구진은 NaturalQuestions-Open 데이터셋을 사용하여 오픈 도메인 질의응답 작업에서 검색기-독자(retriever-reader) 모델의 성능을 분석했습니다.

5.1 실험 구성
- Retriever
	- 목적: 주어진 질문에 관련된 문서를 대규모 문서 집합에서 찾아내는 시스템
	- Contriever: Facebook AI에서 개발한 고성능 검색 모델
	- MS-MARCO: Microsoft에서 만든 대규모 질의응답 데이터셋
	- Contriever를 MS-MARCO을 활용해 fine-tuning
- Reader
	- 목적: Retriever가 찾아낸 관련 문서들을 읽고 질문에 답변하는 언어 모델
	- 이 실험에서는 GPT-3.5-Turbo, Claude-1.3 등 여러 모델을 사용
- 평가 지표
	- Retriever recall
		- 정의: Retriever가 관련 문서를 얼마나 잘 찾아내는지 측정
		- 계산: (검색된 관련 문서 수) / (전체 관련 문서 수)
	- Reader accuracy
		- 정의: Reader 모델이 얼마나 정확하게 질문에 답변하는지 측정
		- 계산: (정확한 답변 수) / (전체 질문 수)
- 실험 방법
	- 검색된 문서의 수(k)를 변화시키면서 Retriever recall과 Reader accuracy를 측정하여, 문서 수 증가에 따른 성능 변화를 분석합니다.
	
5.2 주요 발견
- 성능 포화
	- Reader 모델의 성능은 Retriever recall이 포화되기 훨씬 전에 포화되었습니다.
	- 이는 Reader 모델이 추가로 검색된 문서를 효과적으로 활용하지 못한다는 것을 의미합니다.
- 추가 문서의 제한적 효과
	- 20개 이상의 검색된 문서를 사용하는 것은 Reader 성능을 미미하게 향상시켰습니다.
	- GPT-3.5-Turbo: 약 1.5% 향상
	- Claude-1.3: 약 1% 향상

- 이러한 결과는 단순히 더 많은 입력 프롬프트를 제공하는 것이 항상 더 나은 성능으로 이어지지 않는다는 것을 보여줍니다. 연구진은 다음과 같은 개선 방향을 제안합니다.
	- Effective reranking: 관련 정보를 입력 프롬프트의 시작 부분에 더 가깝게 배치
	- Ranked list truncation: 적절한 경우 더 적은 수의 문서를 검색

1. 연구를 이해하는 데 도움이 되는 개념들 모음
- Lost in the Middle: 긴 입력 프롬프트의 중간 부분에 있는 정보를 모델이 효과적으로 활용하지 못하는 현상을 가리킵니다.
- Recency Bias: 모델이 입력 프롬프트의 후반부(최근)에 있는 정보를 더 잘 활용하는 경향을 의미합니다.
- Primacy Bias: 모델이 입력 프롬프트의 초반부(처음)에 있는 정보를 더 잘 활용하는 경향을 의미합니다.
- U-shaped Performance Curve: 모델의 성능이 관련 정보의 위치에 따라 U자 형태의 곡선을 그리는 현상을 설명합니다. 즉, 정보가 프롬프트의 시작이나 끝에 있을 때 성능이 높고, 중간에 있을 때 낮아지는 패턴을 의미합니다.
- Context Window: 언어 모델이 한 번에 처리할 수 있는 입력 텍스트의 최대 길이를 의미합니다.
- Multi-document Question Answering: 여러 문서를 입력으로 받아 질문에 답하는 태스크를 의미합니다.
- Key-value Retrieval: 주어진 키에 해당하는 값을 찾아내는 태스크를 의미합니다.
- Retrieved Distractor: 질문과 관련은 있지만 정답을 포함하지 않는, 검색 시스템에 의해 선별된 문서를 의미합니다.
- Hard Negatives: 모델이 정답과 구별하기 어려운 오답 후보들을 의미합니다. 이 연구에서는 retrieved distractor를 지칭합니다.
- Query-aware Contextualization: 쿼리(질문)를 입력 프롬프트의 앞뒤에 배치하여 모델이 쿼리를 인식한 상태에서 문맥을 처리하도록 하는 기법입니다.
- Instruction Fine-tuning: 언어 모델을 특정 지시사항이나 태스크에 맞게 추가로 훈련시키는 과정을 의미합니다.
- Closed-book Setting: 모델에게 추가적인 문서나 정보 없이 오직 질문만 주어진 상태에서 답변을 생성하도록 하는 설정을 의미합니다.