2025.03.13

[LinkedIn](https://www.linkedin.com/posts/byeongheon-lee-2b83aa222_%EB%85%BC%EB%A6%AC%EC%A0%81-%EC%B6%94%EB%A1%A0%EC%9D%84-%EC%9C%84%ED%95%B4-%ED%95%84%EC%9A%94%ED%95%9C-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EA%B5%AC%EC%A1%B0%EB%9E%80-%EC%95%84%EB%A7%88-%EC%A0%9C%EA%B0%80-%EC%97%AC%ED%83%9C%EA%B9%8C%EC%A7%80-%EA%B3%B5%EC%9C%A0%ED%95%9C-%EC%97%B0%EA%B5%AC%EB%93%A4-activity-7305875072055922689-t_a4?utm_source=share&utm_medium=member_desktop&rcm=ACoAADfxcywBkH2Mi2-YPZm7jSZERa3dQ2_DDEY)

논리적 추론을 위해 필요한 데이터 구조란?

아마 제가 여태까지 공유한 연구들 중에, 가장 저의 관심사와 가까운 연구인 것 같습니다.

Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs
https://arxiv.org/abs/2503.01307

RL로 효과를 볼 수 있는 모델(Qwen)과 그렇지 못한 모델(Llama) 간의 차이점을 다루고 있습니다. 논리적 추론에 필요한 'Cognitive Behavior(인지적 행동, DeepSeek R1에서 나타난 aha moment와 유사)'을 보이는 경우에만 RL을 적용해 성능을 개선할 수 있고, 이러한 cognitive behavior를 보강해서 추가 학습하면 RL로 인한 성능 개선폭이 확연히 늘어난다고 합니다.     

o1 이후의 inference time scaling 트렌드에서 이렇게 데이터셋에 주목하는 경우가 많은 것 같네요. 그리고 동시다발적으로 '데이터의 구조'에 주목하고 있는 것 같습니다. 노이즈 투성이인 인터넷 데이터를 모으는 대신, 인간이 논리적 사고를 직접 데이터에 녹여내는 것이 당분간의 트렌드가 되지 않을까 싶습니다. 다만 이미 충분히 강한 base model이 있으니, 데이터의 양보다는 개별 데이터가 얼마만큼 '높은 수준의 사고력'을 내포하고 있는 지가 중요해지지 않을까요? 

---

🔍 연구 소개

이 연구는 유사한 크기의 두 LLM이 RL 과정에서 왜 극명하게 다른 성과를 나타내는지 그 원인을 분석합니다. 구체적으로는 Qwen-2.5-3B와 Llama-3.2-3B를 대상으로, 'Countdown'이라는 수학 퍼즐 게임을 통해 모델들의 추론 능력을 평가했습니다. 이는 n 개의 숫자에 사칙연산을 적용해 최종 답을 도출하는 것입니다. 예를 들어 25, 30, 3, 4가 있고 32가 정답이라면 (30 - 25 + 3) x 4 이런 식으로 푸는 거죠.

연구에서 주장하는 바는, base 모델이 ‘cognitive behavior’를 보이는지 여부가 이 격차를 유발한다는 것입니다. 이러한 행동을 내재하고 있어야지만 RL 적용 후 추가로 주어진 inference time을 잘 활용할 수 있다고 합니다. 

LLM이 일반적으로 보이는 선형적인 문제풀이 전략이 아닌, 인간 전문가들에게서 주로 보이는 cognitive behavior를 위주로 확인했습니다;

- Verification: 중간 결과를 확인하는 행동입니다. 예를 들어, "지금까지 나온 결과가 맞는지 다시 확인해보자"와 같은 방식으로 나타납니다.
- Backtracking: 잘못된 접근 방식을 빠르게 포기하고, 다른 방법으로 재시도하는 행동입니다. 예를 들어, "이 방법이 안 되니 다른 방법으로 접근해보자" 같은 형태입니다.
-  Subgoal Setting: 복잡한 문제를 작은 단계로 나누어 각 단계별로 목표를 설정하는 행동입니다.
- Backward Chaining: 목표 결과에서 시작해 처음 입력 값으로 거꾸로 추론하는 행동입니다. "결과가 이렇게 나와야 하니, 이전 단계는 이래야 한다" 식으로 접근합니다.

Base model 상태의 Qwen과 Llama를 분석해보니, Qwen과 달리 Llama 계열은 전반적으로 이러한 행동 빈도가 매우 낮게 목격되었습니다. 이후 동일한 설정으로 RL을 적용하니 Qwen 모델은 출력 길이를 늘리고, 작업 성능이 큰 폭으로 향상되는 반면, Llama 모델은 이 행동들을 거의 보이지 않았으며 성능 개선이 더뎠습니다.

Llama가 이렇게 부진한 이유는 pre training에 사용한 오픈소스 데이터셋의 구성에 있는 것으로 보입니다. 유명 수학 데이터셋들(OpenWebMath, FineMath 등)을 분석한 결과, 실제로 인지적 행동들이 포함된 예시는 극히 적다는 점을 발견했습니다. 이는 곧 대규모 데이터가 존재하더라도, 그 안에 ‘올바른 논리적 추론 패턴’을 보여주는 사례가 부족하다면 모델은 이를 학습하지 못할 가능성이 높다는 것을 시사합니다. 

그 후 이런 행동을 주입할 방법을 실험했습니다. 

두 모델에게 인위적으로 행동을 주입(priming)하기 위해 'Backtracking만 써서 문제를 푸는 예시' 등을 소규모로 SFT 적용하니, RL 효과가 크게 향상되었습니다. 두 모델 간 성능 차이는 미미했으며, 학습 데이터의 정답이 틀렸더라도 안에 올바른 과정만 담겨 있으면 충분히 상승 효과가 생겼죠. 이를 두고 “그냥 컴퓨터 자원을 더 할당해서 나타난 결과가 아닌가?”라는 의문이 생겨, 아무런 내용이 없는 빈 CoT나 토큰 길이만 맞춘 무의미한 CoT 등을 비교 실험했고, 실제 ‘행동 패턴’이 포함되지 않으면 같은 효과가 나타나지 않음을 확인했습니다.

또한, 사전학습 데이터 자체를 재구성해 continued pretraining 하는 방식도 시도했습니다. OpenWebMath에서 인지적 행동이 분명히 드러난 문서들만 골라 Llama를 학습시켰더니 Qwen과 유사한 수준으로 RL 성능이 향상된 반면, 그런 행동이 거의 없는 문서만으로 학습한 경우에는 성능이 빨리 정체되었습니다. 결국 모델에게 '해답을 찾기 위해 필요한 행동 패턴'을 얼마나 자주 노출시키느냐가 훨씬 중요하다는 결론을 내릴 수 있습니다.

이번 연구에서는 4가지 행동 패턴만 다뤘지만, pretraining 데이터에 녹아든 더 다양한 cognitive behavior들이 규정되는 것은 당연한 수순이겠죠. '불확실성(uncertainty)을 표현하기', 'A에 B라는 특성이 있음이라는 명제에서 B가 A의 특성임을 유추하기(reversal curse)' 등 기존에 LLM의 한계로 규정된 현상들을 명시적으로 학습 데이터에 포함하는 것으로 inferene time scaling을 더 가속시킬 수 있지 않을까요?