[Facebook](https://www.facebook.com/groups/agikr/permalink/2281451218862590/)

최근에 Chatbot Arena를 운영하고 있는 LMSYS가 Arena Hard v0.1이라는 새로운 벤치마크를 발표했습니다.
https://lmsys.org/blog/2024-04-19-arena-hard/

기존의 벤치마크들을 이용한 평가방법은 점수가 상향평준화되어 비교하기 어려운 점, 데이터 오염이 의심되는 점 때문에 회의론만 늘어가고 있었습니다.
Chatbot Arena는 2개의 익명화된 모델에게 유저가 직접 질문을 하고, 이에 대한 답변을 평가하는 방식이라 기존의 벤치마크에 비해 신뢰할만 하다는 평가를 받고 있었습니다.  
이를 통해 수집한 200,000개의 유저 쿼리를 기반으로 500개의 질문을 Arena Hard v0.1이라는 이름으로 내놓았는데요, 다음과 같은 부분을 흥미롭게 봤습니다.

1) 압도적 1위를 차지한 GPT-4-Turbo-2024-04-09에 이어, Claude 3 Opus, GPT-4-0314, Claude 3 Sonnet, Claude 3 Haiku, Llama-3-70B-Instruct가 상위권을 차지하고 있습니다.
2) 해당 벤치마크의 답은 모두 GPT Turbo가 채점하고 있습니다. Claude Opus를 사용해 채점할 시 Claude 모델들의 점수가 월등히 올라갔습니다. 다만 GPT Turbo가 실제 LMSYS의 결과, 즉 인간의 평가와 더 유사하기 때문에 해당 벤치마크에서는 GPT Turbo에 의한 bias가 있을 수 있다는 점만 명시하고 여전히 채점자로 활용하고 있다고 합니다.
3) 더 긴 답변이 전반적으로 높은 점수를 받을 수 있다고 합니다.

특정 LLM을 채점자로 사용해 bias가 있을 수 있다는 점을 제외하고는, 여러모로 LLM 벤치마크의 귀감이 될 사례라고 생각합니다.

벤치마크와는 다른 이야기이지만, '실시간으로 유저로부터 데이터를 수집해 LLM 개발/평가에 도움을 주는 시스템'에도 눈이 갔습니다. 
최근 Llama3 모델들을 공개한 Meta는 15t tokens를 활용한 pretraining + 10M human-annotated data를 활용한 optimization에도 불구하고 "개선의 여지가 있다"고 평가했습니다. 지금과 같이 실제 유저가 많아진 상황에서는 training cost를 높여서라도 inference cost를 낮추는 것이 중요하고, 이를 위해서는 8b와 같은 작은(?) 모델들에게도 천문학적인 데이터가 요구될 것 입니다(https://twitter.com/karpathy/status/1781028605709234613). 
앞으로 고품질 데이터의 중요성은 더욱 올라갈 것으로 보입니다. X, Reddit, Facebook, Instagram과 같은 SNS 플랫폼들은 앞으로 광고 사업을 줄이고 고품질 텍스트/이미지/비디오/오디오 데이터만 정제해서 팔아도 수익을 낼 수 있겠다는 생각이 드네요(데이터의 권리가 누구에게 있는지를 우선 확실히 해야겠지만요).

SNS처럼 소수의 데이터를 위해 대규모 트래픽을 감수하는 구조 대신, 처음부터 소수의 "고품질 데이터"를 수집하기 위해 디자인된 공간이 나올수도 있겠다는 생각이 듭니다. 하나의 전문적인 주제에 대해 소수의 전문가 집단이 의견을 교환하는 과정을 처음부터 끝까지 기록하는 등... 모든 레퍼런스에 대해서 일일이 입력하기는 어려우니, 전문가 유저가 키워드만 입력하면 해당 공간에 상주하는 AI 에이전트가 가장 적합한 레퍼런스를 찾아오는 식으로 보조할 수도 있겠네요. 전문성있는 유저들을 지속적으로 참여시킬 유인을 제공하는 것이 관건이겠지만, Wikipedia나 Hackernews와 같은 사례를 보면 아예 불가능할 것 같지는 않습니다. 

전문성은 떨어져도 특정 상황속 인간의 반응을 수집하기 위해서는 전세계에서 다양한 유저가 플레이하는 AAA 게임과 연계해 AI NPC와 유저의 상호작용 데이터를 수집하는 것도 재미있는 시도가 될 것 같습니다. 

최대규모 유튜브 음성 transcript (https://twitter.com/Dorialexander/status/1780959636306481476), 15t token web data(https://twitter.com/gui_penedo/status/1781953413938557276) 등 어마어마한 규모의 데이터가 공개되고는 있지만, 보다 전문성있고 다양한 데이터를 실시간으로 수집하고자 하는 수요는 여전히 클 것 같습니다. 

   