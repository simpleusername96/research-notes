2024.12.03

[LinkedIn](https://www.linkedin.com/posts/byeongheon-lee-2b83aa222_ai-prompt-engineering-a-deep-dive-activity-7269420781686669312-krlR?utm_source=share&utm_medium=member_desktop)
[Facebook](https://www.facebook.com/share/p/15diZQYz9m/)
[Threads](https://www.threads.net/@gandanhanid/post/DDFiKo9T25E?xmt=AQGzVizHDFqBpjC2l7tP-rMvK3qSadOG21isynS1k6ynfw)

프롬프트 엔지니어링을 주제로 Anthropic의 팟캐스트를 들었는데 흥미로운 내용이 많았습니다. 프롬프트와 씨름한 경험을 가진 분이라면 공감할 만한 내용이 많았고, Anthropic에서조차도 프롬프트를 작성할 때 수많은 trial and error를 겪으면서 고통받는다는 것을 듣고 내적 친밀감도 생겼네요. 전반적으로 추상적인 이론이나 복잡한 방법론보다는 쉽게 풀어서 말하는 것과 지속적인 반복 작업을 중요시 생각하는 점이 돋보였습니다. 인상깊었던 내용 위주로 내용을 간략히 정리했으니, 관심있으신 분은 한 번 시청하시는 것 추천드립니다!

https://www.youtube.com/watch?v=T9aRN5JkmL8

1. 프롬프트 엔지니어링은 반복의 기술이다
- 프롬프트 엔지니어링을 통해 좋은 프롬프트를 찾아내는 것 이상으로, 출력과 결과를 기반으로 반복적으로 개선하는 과정을 체화하는 것이 중요합니다.
- 프롬프트가 예상치 못한 입력을 처리할 때의 결과를 테스트하며, 명확하지 않은 상황에 대한 대응 방안을 추가로 정의해야 합니다. 예를 들어, 이름이 들어와야 할 자리에 숫자가 들어오거나 입력값이 비어 있을 때의 처리 방법을 미리 정의하면 모델이 거짓된 판단을 내리는 것을 방지할 수 있습니다.
- 모든 문제를 해결할 "완벽한 프롬프트"를 찾으려는 유혹에 빠지기 쉽습니다. 그러나 실무에서는 과도한 반복보다는 적절한 시점에서 타협하는 것이 중요합니다. 
- LLM이 특정 작업에서 지속적으로 실패한다면, 개선에 매진하기 보다 다음 모델을 기다리는 것이 현명할 수도 있습니다.
- 연구와 실무는 프롬프트 설계의 목표가 다릅니다. 연구에서는 모델의 가능성과 유연성을 탐구하기 위해 추상적이고 비유적인 예시를 비교적 적게 사용하며 응답의 다양성을 중요시합니다. 반면 실무에서는 유저가 원하는 대답을 일관되게 출력하는 것이 핵심입니다. 그렇기 때문에 명확한 구조를 가진 다수의 예시를 포함해 다양한 입력에서도 안정적인 결과를 얻는 데 초점을 맞춰야 합니다.

1. 프롬프트 엔지니어링은 사고를 외부화하는 작업이다
- 모델에게 작업을 설명할 때는 내가 알고 있는 모든 정보를 명시적으로 전달해야 합니다. 우리가 무의식적으로 사용하는 가정이나 배경지식은 모델이 이해하지 못할 수 있습니다. 이를 체계적으로 풀어내어 필요한 정보를 완전히 외부화해야 합니다. 
- 프롬프트는 사람에게 복잡한 작업을 설명하는 것처럼 작성하는 것이 효과적입니다. 모델의 입장에서 "이 지침이 명확한가?"를 생각하며 수정하면 더 나은 결과를 얻을 수 있습니다.
- 프롬프트가 불분명한 경우 모델 자체에 피드백을 요청하는 것도 방법입니다. "이 지침에서 모호한 부분이 무엇인지 알려달라"거나 "오류가 발생한 이유와 개선 방안"을 모델에게 직접 물어보는 것도 효과적입니다. 

1. Pretrained 모델 프롬프팅
- Pretrained 모델은 RLHF 모델과 매우 다릅니다. Pretrained 모델의 출력은 입력의 스타일을 그대로 반영하는 경향이 강하며, 입력에 오타가 있다면 출력에서도 유사한 오타가 나타납니다. 그렇기 때문에 두 모델의 프롬프팅 전략에는 상당한 차이가 있습니다.
- RLHF 모델은 훈련 과정에서 구체적인 목표를 학습했기 때문에 복잡한 비유보다는 직설적인 작업 지시가 더 효과적입니다.
- Pretrained 모델은 "텍스트 중간에 떨어진 느낌"으로 작업을 이어갑니다. 따라서 프롬프트를 작성할 때 모델이 예상할 수 있는 "자연스러운 흐름"을 제공하는 것이 중요합니다.
- RLHF 모델은 깔끔하고 정제된 출력을 생성하도록 조정되었지만, Pretrained 모델은 보다 자유로운 답변을 생성합니다. 이런 관점에서  Pretrained 모델은 실제 사용자의 입력 프롬프트 생성 등의 작업에 더 적합합니다.

1. 학습 데이터와 프롬프트 설계
- 어떤 유저들은 LLM이 학습한 데이터의 특성을 추측하여 프롬프트에 반영하는 경향이 있습니다. 
- 예를 들어, "모델이 고등학교 퀴즈를 더 많이 접했을 것"이라고 추측해 퀴즈 스타일로 프롬프트를 작성하거나 모델의 학습 데이터에서 많이 등장했을 법한 언어 패턴을 반영해 설계하는 경우가 자주 있습니다.
- 그러나 오늘날 RLHF가 적용된 모델은 pretrain 학습 데이터 패턴에 덜 의존하는 경향이 있습니다. 학습 데이터를 고려하되, 이를 과대평가하여 프롬프트를 설계하지 않도록 주의가 필요합니다.

1. Chain of Thought
- CoT를 "진정한 추론"으로 볼 것인지는 논란의 여지가 있지만, 그 효과는 부인할 수 없습니다. 단순히 작업을 수행하도록 지시하는 것보다, 모델에게 사고 과정을 설명하도록 요청했을 때 더 나은 성능을 보입니다.
- 수학 같은 특정 영역에서는 CoT가 모델 학습 과정에 통합되었습니다. 과거에는 "Let's think step by step"과 같은 프롬프트를 명시적으로 제공해야 정확도가 높아졌지만, 현재는 고급 모델이 명시적인 지시 없이도 자연스럽게 단계별 추론을 수행하도록 설계되었습니다.
- CoT의 흥미로운 점은 중간 단계에서 오류가 발생하더라도 모델이 종종 올바른 최종 답변에 도달한다는 것입니다. 이는 CoT가 의사결정을 위한 발판 역할을 하여 문제 해결 과정을 강화한다는 것을 시사합니다.
- CoT의 효과가 단순히 "더 많은 토큰을 출력"해서 발생한다는 시각도 있지만, 영상에서는 이를 부정적으로 받아들이고 있습니다. 예를 들어 모델에게 "100 토큰 동안 무작위 단어를 반복한 후 답변하라"고 요청했을 때는 명확한 추론 과정을 유도했을 때보다 훨씬 부정확한 결과를 도출했습니다. 