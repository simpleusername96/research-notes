2025.04.01

[LinkedIn](https://www.linkedin.com/posts/byeongheon-lee-2b83aa222_this-is-why-large-language-models-can-understand-activity-7312522838429560833-8p1A?utm_source=share&utm_medium=member_desktop&rcm=ACoAADfxcywBkH2Mi2-YPZm7jSZERa3dQ2_DDEY)

안녕하세요,

유튜브에서 LLM이 왜 기존 ML 관점에서 우려했던 과적합(overfitting)을 잘 피해가는지에 대해 설명하는 영상을 봤는데 흥미롭더라구요. 이를 처음 접하시는 분들에게 추천드립니다. 이해한 내용과 느낀점을 간략히 공유해 보려고 합니다.

https://www.youtube.com/watch?v=UKcWu1l_UNw

1. 고전적 관점: 모델이 커지면 과적합 위험 증가

기존 머신러닝에서는 모델 파라미터 수가 데이터 포인트에 비해 너무 크면 훈련 데이터의 세세한 노이즈까지 학습해버려 새로운 데이터에는 잘 작동하지 못하는 과적합 문제가 발생하기 쉽다고 봤습니다. 모델이 훈련 데이터를 거의 '암기'해버려서, 실제 풀어야 할 문제의 일반적인 패턴을 놓치는 거죠. 그래서 모델의 파라미터 수를 데이터 양에 비해 너무 크지 않게 제한하는 것이 일반화 성능을 높이는 중요한 전략으로 여겨졌습니다. 하지만 최근의 LLM 개발 트렌드에서는 이러한 지적이 유의미하지 않은 것으로 보입니다. 

2. 반전: Double Descent 현상

이는 2019년 발표된 "Reconciling modern machine learning practice and the bias-variance trade-off" (arxiv.org/abs/1812.11118) 같은 연구들에서 주목받기 시작한 Double Descent 현상으로 어느 정도 설명할 수 있습니다. 파라미터 수가 커지면 일정 수준까지는 기존의 관점대로 과적합으로 인해 테스트 성능이 저하됐지만, 이를 더 늘리자 오히려 테스트 성능이 다시 좋아지는 모습이 관찰되었습니다. 

3. 그럼 왜? "복권 당첨" 같은 일이 벌어진다! (Lottery Ticket Hypothesis)
    
그렇다면 왜 이런 현상이 나타날까요? 영상에서는 "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks" (https://arxiv.org/abs/1803.03635)를 중심으로 설명합니다. 이 연구에서는 모델 파라미터를 극단적으로 줄여도 성능이 어느 정도 유지되는 것을 보였습니다. 그리고 이를 설명하기 위한 'Lottery Ticket Hypothesis'를 제시했습니다. 

이 가설에 따르면 아주 큰 신경망 안에는 수많은 작은 '서브네트워크'들이 존재하는데, 마치 복권처럼 각기 다른 초기 가중치 값을 갖습니다. 이 거대한 네트워크를 훈련시키면 마치 수많은 복권 중 '당첨된' 소수의 서브네트워크, 즉 우연히 좋은 초기값을 가져 학습이 빠르고 효율적으로 진행되는 네트워크를 찾아내는 것과 유사한 결과가 나온다는 것입니다. 모델이 클수록 이런 '복권'이 많아져 좋은 서브네트워크를 찾을 확률이 높아지고, 이것이 대규모 모델이 과적합을 피하고 오히려 일반화 성능이 좋아지는 이유 중 하나라는 것입니다.

4. 영상 후반부 및 마무리
    
영상 뒷부분에서는 이 개념을 확장해서, 스케일링을 통해 LLM이 계속 발전하면 언젠가 AGI에 도달할 수 있다는 식의 추측으로 마무리합니다. 아무래도 정식적인 ML 백그라운드가 없다 보니, 다른 전공자분들에게는 익숙할 수 있는 이런 내용들이 새롭게 다가오네요. 

외국 유튜브 채널들을 보면 이렇게 깊이 있는 주제에 대해 시각 자료와 쉬운 설명을 곁들인 영상이 나오고, 댓글 창에서는 영상 내용에 대한 활발한 논의가 이어지는 것을 자주 볼 수 있습니다. 이 영상 댓글에서도 언급한 연구보다 선행된 연구가 있다는 지적이나, 영상 후반부의 AGI 주장에 대한 논리적인 반박 등을 찾아볼 수 있었습니다. 이처럼 건설적으로 토론하는 문화가 국내 커뮤니티에도 더 활발해지면 좋겠다는 생각을 해봅니다.

긴 글 읽어주셔서 감사합니다!